# kafka-market-stream
## CI Status
[![CI](https://github.com/MichaelG-create/kafka-market-stream/actions/workflows/ci.yml/badge.svg)](https://github.com/MichaelG-create/kafka-market-stream/actions/workflows/ci.yml)

kafka-market-stream is an end‑to‑end streaming prototype that ingests market index data into Kafka, persists it to DuckDB, and exposes live dashboards in Grafana, with CI smoke tests on GitHub Actions.

***

## Overview

This project simulates a real‑time **market indices** feed (SP500, STOXX600, NIKKEI225) and demonstrates a full data path from CSV → Kafka → DuckDB → HTTP API → Grafana dashboards.  
It is designed as a portfolio piece for data engineering roles, with clean separation between domain, application services, infrastructure, monitoring, and CI.

---

## Architecture

The repository is structured as a Python package `market_streaming` with application services, ports, and infrastructure adapters, plus an API and Grafana configuration.

- **Producer path**:  
  - `data/indices_sample.csv` contains synthetic index ticks generated by `data/generate_sample_data.py`.  
  - `src/market_streaming/main_producer.py` builds a `MarketTickProducerService` using `CsvTickSource` and `ConfluentKafkaTickPublisher` to send JSON messages into Kafka topic `market_indices_raw` (overridable via `KAFKA_TOPIC`).

- **Consumer path**:  
  - `src/market_streaming/main_consumer.py` builds a `MarketTickConsumerService` wired with `ConfluentKafkaMessageConsumer`, `DuckDBTickRepository`, `DuckDBMetricsRepository`, and `JsonFileLogger`.  
  - Messages are read from Kafka, converted to `MarketTick` domain objects, inserted into DuckDB table `market_ticks`, and run‑level metrics are written into `pipeline_metrics`.

- **Storage & analytics**:  
  - `DuckDBTickRepository` and `DuckDBMetricsRepository` persist data into `data/market_data.duckdb` or a custom path via `MARKET_DB_PATH`.  
  - `data/duckdb_once_setup_grafana.py` adds schema extras (for example `timestamp_received`, `market_metrics` and `live_ticks` views) used by dashboards.

- **API & dashboards**:  
  - `api/duckdb-api.py` exposes `/metrics` and `/live` endpoints backed by DuckDB queries, returning JSON enriched with a `time` field.  
  - `docker-compose.grafana.yml` defines a `grafana` service and a `duckdb-api` service; Grafana dashboards in `data/grafana_dashboard1_settings.json` and `data/grafana_dashboard2_settings.json` use the Infinity data source to visualize prices, volumes, and summary metrics.

- **DevOps & CI**:  
  - `.github/workflows/ci.yml` runs linting (ruff, black, pylint) and a smoke integration test against a Kafka service on every push.  
  - `tests/test_pipeline_integration.py` runs `./scripts/run_pipeline.sh`, then validates that DuckDB contains exactly 150 rows in `market_ticks`, with variants that assume Kafka is running or auto‑launch Kafka via Docker.

***

## Tech Stack

- **Streaming & messaging**: Apache Kafka via `confluent-kafka` producer/consumer wrappers.  
- **Compute & language**: Python application package `market_streaming` using dataclasses, Protocol‑based ports, and type hints.  
- **Storage**: DuckDB as embedded OLAP store for `market_ticks` and `pipeline_metrics`.  
- **Web/API**: Flask REST API exposing `/metrics` and `/live` for Grafana (CORS enabled via `flask-cors`).  
- **Dashboards**: Grafana with Yesoreyeram Infinity datasource, reading JSON from the DuckDB API to create timeseries and table panels.  
- **Tooling & quality**: `ruff`, `black`, and `pylint` in CI, with `pytest` integration tests and `uv` as the environment/installer in GitHub Actions.

***

## Setup and Usage

### 1. Prerequisites

- Python 3.x with ability to install dependencies defined in the package requirements (confluent-kafka, duckdb, pandas, flask, matplotlib, etc.).  
- Local Kafka broker (for example Docker container) reachable at `localhost:9092`, or use the provided scripts/tests that can start Kafka via Docker.  
- Docker and Docker Compose to run Grafana plus the DuckDB API (`docker-compose.grafana.yml`).

### 2. Install Python dependencies

From the project root:

```bash
# Using uv (aligned with CI)
uv pip install -r src/kafka_market_stream.egg-info/requires.txt
```

This installs the core runtime dependencies for the producer, consumer, DuckDB, API, and optional notebook tooling.

### 3. Generate sample market data

```bash
python data/generate_sample_data.py
```

This creates `data/indices_sample.csv` with 150 rows (50 per symbol) of timestamped price and volume data.

### 4. Start Kafka

You can run Kafka however you prefer; the CI assumes an `apache/kafka:latest` container with port `9092` exposed.  
Ensure Kafka is reachable at `localhost:9092` before starting the producer and consumer, or rely on `scripts/run_pipeline.sh` for orchestration if present.

### 5. Run the streaming pipeline locally

Typical manual flow:

```bash
# 1) Run producer – stream CSV → Kafka
PYTHONPATH=src python -m market_streaming.main_producer

# 2) Run consumer – Kafka → DuckDB, metrics + logs
PYTHONPATH=src python -m market_streaming.main_consumer
```

- Producer reads from `data/indices_sample.csv`, publishes JSON payloads to topic `market_indices_raw` (or `KAFKA_TOPIC`).  
- Consumer polls Kafka, writes rows into DuckDB (`market_ticks`) and run metrics into `pipeline_metrics`, with JSON logs written to `logs/pipeline.log`.

Environment overrides:

```bash
export MARKET_DB_PATH=/path/to/custom.duckdb
export KAFKA_TOPIC=my_custom_topic
```

These variables are respected by both the repositories and the integration tests.

### 6. Prepare DuckDB schema for Grafana (optional)

```bash
python data/duckdb_once_setup_grafana.py
```

This adds `timestamp_received`, a `market_metrics` view aggregating tick statistics by symbol, and a `live_ticks` view limiting to the latest records.

***

## Grafana Dashboards

### 1. Start Grafana + DuckDB API

From the project root:

```bash
docker compose -f docker-compose.grafana.yml up --build
```

This starts:

- `duckdb-api` on port `8080`, exposing:
  - `/metrics` with `symbol`, `tick_count`, `avg_price`, `max_price`, `min_price`, plus a `time` column added in Python.  
  - `/live` returning a wide time‑series of SP500/STOXX600/NIKKEI225 prices and volumes for recent timestamps shared by all symbols.
- `grafana` on port `3000`, mounting `./data` as read‑only, where dashboard JSON configurations live.

### 2. Dashboards

The two JSON dashboards demonstrate different views over the same data:

- `data/grafana_dashboard1_settings.json` – a **Market Data Live Dashboard** with:
  - Timeseries of live prices for SP500, STOXX600, NIKKEI225 based on `/live`.  
  - A metrics table summarizing tick counts and average prices via `/metrics`.

- `data/grafana_dashboard2_settings.json` – a **Global Markets Live Dashboard** with:
  - Smoothed price trend lines, stacked volume charts, and stat panels per symbol.  
  - A summary table with min/max/avg prices per symbol plus conditional formatting.

Both dashboards rely on the Infinity datasource pointing to `http://duckdb-api:8080/{live|metrics}` inside the Docker network.

***

## Tests and CI

The CI workflow runs in two stages on every push to `main` or on pull requests.

- **Lint & quality job**:  
  - Installs `uv`, then adds dev tools `ruff`, `black`, and `pylint`.  
  - Runs `ruff check`, `ruff format --check`, `black --check .`, and `pylint` against `market_streaming` and `tests`.

- **Smoke-tests job**:  
  - Starts an `apache/kafka:latest` service with health checks on port `9092`.  
  - Runs `python -m pytest tests/test_pipeline_integration.py::test_pipeline_smoke -v` with `PYTHONPATH=src`.

The integration tests perform two main checks:

- `test_pipeline_smoke`: assumes Kafka is already running, executes `./scripts/run_pipeline.sh`, and asserts that the pipeline logs `"SUCCESS"` and DuckDB contains 150 rows in `market_ticks`.  
- `test_pipeline_kafka_autolaunch`: uses the Docker SDK to stop any running Kafka containers, verifies port 9092 is closed, runs `run_pipeline.sh` to auto‑launch Kafka, then checks again for `"SUCCESS"` and `150` rows.

***

## Monitoring & Metrics

The consumer records run‑level pipeline metrics and structured logs to support a monitoring mindset.

- **Run metrics**:  
  - `RunMetrics` dataclass carries `run_started_at`, `run_ended_at`, `elapsed_seconds`, `messages_processed`, `errors`, and `max_timestamp`.  
  - `DuckDBMetricsRepository` persists each run into DuckDB table `pipeline_metrics`, which can be queried or visualized in Grafana in a future dashboard.

- **Structured logs**:  
  - `JsonFileLogger` writes JSON lines to `logs/pipeline.log` with fields such as event name (`pipeline_start`, `pipeline_end`), timestamps, counters, and error details.  
  - `MarketTickConsumerService` logs message processing errors and emits start/end events with metrics, aligned with the monitoring user stories in the backlog.

Future Grafana panels can use `pipeline_metrics` for:

- Pipeline throughput over time (messages per run).  
- Error rate across runs.  
- Run duration trends to detect performance regressions.

***


